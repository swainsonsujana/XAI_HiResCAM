from IPython.display import Image, display
import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
import keras
import nibabel as nib
from scipy import ndimage
import os
import zipfile
import numpy as np
import tensorflow as tf  # for data preprocessing
import keras
from keras import layers
from keras import regularizers
from scipy import ndimage
from tensorflow.python.keras import regularizers

from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau

def normalize_map(volume):
    """Normalize the volume"""
    min = 0
    max = 1.5
    volume[volume < min] = min
    volume[volume > max] = max
    volume = (volume - min) / (max - min)
    volume = volume.astype("float32")
    #volume = (volume/255.0)*1.5
    return volume
def resize_map(img):
    """Resize across z-axis"""
    # Set the desired depth
    desired_depth = 60
    desired_width = 128
    desired_height = 128
    # Get current depth
    current_depth = img.shape[2]
    current_width = img.shape[0]
    current_height = img.shape[1]
    # Compute depth factor
    depth = current_depth / desired_depth
    width = current_width / desired_width
    height = current_height / desired_height
    depth_factor = 1 / depth
    width_factor = 1 / width
    height_factor = 1 / height
    # Rotate
    #img = ndimage.rotate(img, 90, reshape=False)
    # Resize across z-axis
    img = ndimage.zoom(img, (width_factor, height_factor, depth_factor), order=1)
    return img

#fetch autistic images from database
autisticpaths= [
    os.path.join(os.getcwd(), "/kaggle/input/resampled/resampled/resampled-aut", x)
    for x in os.listdir("/kaggle/input/resampled/resampled/resampled-aut")
    ]
print("Number of scans: " + str(len(autisticpaths)))

#fetch control images from database
nonautisticpaths = [
    os.path.join(os.getcwd(), "/kaggle/input/resampled/resampled/resampled-nonaut", x)
    for x in os.listdir("/kaggle/input/resampled/resampled/resampled-nonaut")
]
print("Number of scans: " + str(len(nonautisticpaths)))

#Read and process the scans.
# Each scan is resized across height, width, and depth and rescaled.
autistic_scans = np.array([process_scan(path) for path in autisticpaths])
nonautistic_scans = np.array([process_scan(path) for path in nonautisticpaths])

autistic_labels = np.array([1 for _ in range(len(autistic_scans))])
nonautistic_labels = np.array([0 for _ in range(len(nonautistic_scans))])

scans=np.concatenate((autistic_scans,nonautistic_scans),0)
labels=np.concatenate((autistic_labels,nonautistic_labels),0)

# Split data in the ratio 73-27 for training and validation.
from sklearn.model_selection import train_test_split
x_train, x_val, y_train, y_val = train_test_split(scans, labels, test_size=0.27, random_state=1, shuffle=True)
print(
    "Number of samples in train and validation are %d and %d."
    % (x_train.shape[0], x_val.shape[0])
)

import random
from scipy import ndimage
import torch.functional as F

def train_preprocessing(volume, label):
  volume = tf.expand_dims(volume, axis=3)
  return volume, label

def validation_preprocessing(volume, label):
    #Process validation data by only adding a channel.
    volume = tf.expand_dims(volume, axis=3)
    return volume, label

# Define data loaders.
train_loader = tf.data.Dataset.from_tensor_slices((x_train, y_train))
validation_loader = tf.data.Dataset.from_tensor_slices((x_val, y_val))
batch_size = 4
train_dataset = (
    train_loader.shuffle(len(x_train))
    .map(train_preprocessing)
    .batch(batch_size)
    .prefetch(2)
)
validation_dataset = (
    validation_loader.shuffle(len(x_val))
    .map(validation_preprocessing)
    .batch(batch_size)
    .prefetch(2)
)
